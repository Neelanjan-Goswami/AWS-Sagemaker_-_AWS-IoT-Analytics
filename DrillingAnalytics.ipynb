{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img src=\"https://s3.amazonaws.com/iotanalytics-templates/Logo.png\" style=\"float:left;\">\n",
    "    <h1 style=\"color:#1A5276;padding-left:115px;padding-bottom:0px;font-size:28px;\">Serverless Industry 4.0 & AI: Drive Business Insights from Machine Data</h1>\n",
    "</p>\n",
    "<p style=\"color:#1A5276;padding-left:117px;position:absolute;margin-top:-10px;font-style:italic;font-size:18px;padding-bottom:20px\">\n",
    "By Markus Bestehorn</p>\n",
    "<p style=\"clear:all\"><br/></p>\n",
    "<p>This notebook is used for the re:invent 2018 workshop IOT402. This Notebook will use sample data to create a ML model that can be deployed on AWS Greengrass with AWS Greengrass ML Inference.</p>\n",
    "\n",
    "<p>We'll make use of `gluon.nn.Sequential` to build a neural network that classifies data into two classes: Abnormal drilling processes and normal drilling processes.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Setup\n",
    "First we'll import the necessary bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "import logging\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import tarfile\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to set the contexts for our data and our models. MXNET can use GPUs for preprocessing data as well as training and inference. In general, GPUs will perform better for these tasks as the underlying  mathematical operations for these steps are faster on a GPU. Since this example is fairly small and does not require huge amounts of data, you can still use a CPU. The following code will determine if MXNET can you a GPU in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using CPU\n",
      "INFO:__main__:cpu(0)\n"
     ]
    }
   ],
   "source": [
    "from subprocess import CalledProcessError\n",
    "ctx = None\n",
    "try:\n",
    "    if (mx.test_utils.list_gpus()):\n",
    "        logger.info(\"Using GPU\")\n",
    "        ctx = mx.gpu() \n",
    "    else:\n",
    "        ctx = mx.cpu()\n",
    "        logger.info(\"Using CPU\")\n",
    "except CalledProcessError:\n",
    "    ctx = mx.cpu()\n",
    "    logger.info(\"Using CPU\")\n",
    "finally:\n",
    "    data_ctx = ctx\n",
    "    model_ctx = ctx\n",
    "    logger.info(model_ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is ***optional***: Some of the steps in the following have a random component. For instance, the initialization of the neural network will be done by assigning random values to the parameters. The following step will set the random value generator seeds to constant values. This means the random generators will always generate the same sequence of numbers and therefore the execution of the notebook will always result in exactly the same result, which may be desirable from a reproduction point of view. If you think that you can cope with some of the values in the following having slightly different values, feel free to skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading the dataset\n",
    "This section explores the data set and gives some insights into their structure. First, we will load a set of header names that the CSV files use. These make it easier to access the data by (column) name. The last part of the following code defines three sets of attributes:\n",
    "* The observation features contains only a single attribute which is the `HasAnomaly` attribute of the input data. This value indicates whether the corresponding drilling process was \"normal\" (0) or \"abnomal\" (1). This is the value we want to predict.\n",
    "* The train attributes are all the attributes that contain sensor values or any other values that are measures. We will use these features to train a model. Note that these features DO NOT contain the observation.\n",
    "* The viewing attributes are the union of the two aforementioned attibute sets and only used for viewing purposes, so we can see data more easily in a single view. This set has no functional value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_FLAG_KEY = \"HasAnomaly\"\n",
    "AVG_PRESSURE_KEY = \"AvgPressure\"\n",
    "AVG_MOTOR_KEY = \"AvgMotor\"\n",
    "AVG_SPINDLE_KEY = \"AvgSpindle\"\n",
    "DURATION_KEY = \"duration\"\n",
    "WORK_START_TIMESTAMP_KEY = \"WorkStartTimestamp\"\n",
    "WORK_END_TIMESTAMP_KEY = \"WorkEndTimestamp\"\n",
    "WORK_START_TIME_KEY = \"WorkStart\"\n",
    "WORK_END_TIME_KEY = \"WorkEnd\"\n",
    "ANOMALY_DURATION_LONG_KEY = \"DurationTooLong\"\n",
    "ANOMALY_DURATION_SHORT_KEY = \"DurationTooShort\"\n",
    "ANOMALY_PRESSURE_HIGH_KEY = \"PressureHigh\"\n",
    "ANOMALY_PRESSURE_LOW_KEY = \"PressureLow\"\n",
    "ANOMALY_MOTOR_HIGH_KEY = \"MotorHigh\"\n",
    "ANOMALY_MOTOR_LOW_KEY = \"MotorLow\"\n",
    "ANOMALY_SPINDLE_HIGH_KEY = \"SpindleHigh\"\n",
    "ANOMALY_SPINDLE_LOW_KEY =\"SpindleLow\"\n",
    "\n",
    "\n",
    "CSV_HEADERS = []\n",
    "CSV_HEADERS.append(ANOMALY_FLAG_KEY)\n",
    "CSV_HEADERS.append(AVG_PRESSURE_KEY)\n",
    "CSV_HEADERS.append(AVG_MOTOR_KEY)\n",
    "CSV_HEADERS.append(AVG_SPINDLE_KEY)\n",
    "CSV_HEADERS.append(DURATION_KEY)\n",
    "CSV_HEADERS.append(WORK_START_TIMESTAMP_KEY)\n",
    "CSV_HEADERS.append(WORK_END_TIMESTAMP_KEY)\n",
    "CSV_HEADERS.append(WORK_START_TIME_KEY)\n",
    "CSV_HEADERS.append(WORK_END_TIME_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_DURATION_LONG_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_DURATION_SHORT_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_PRESSURE_HIGH_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_PRESSURE_LOW_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_MOTOR_HIGH_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_MOTOR_LOW_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_SPINDLE_HIGH_KEY)\n",
    "CSV_HEADERS.append(ANOMALY_SPINDLE_LOW_KEY)\n",
    "\n",
    "OBSERVATION_FEATURES = [ANOMALY_FLAG_KEY]\n",
    "TRAIN_ATTRIBUTES = [AVG_PRESSURE_KEY, AVG_MOTOR_KEY, AVG_SPINDLE_KEY, DURATION_KEY]\n",
    "VIEWING_ATTRIBUTES = OBSERVATION_FEATURES + TRAIN_ATTRIBUTES\n",
    "\n",
    "TRAIN_FEATURE_COUNT = len(TRAIN_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the CSV into and print some details about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:loaded data from AWS IoT Analytics dataset 'classified_data'\n",
      "INFO:__main__:Shape of complete matrix: (7364, 5)\n",
      "INFO:__main__:Shape of feature matrix: (7364, 4)\n",
      "INFO:__main__:Shape of observation: (7364, 1)\n"
     ]
    }
   ],
   "source": [
    "# specify dataset\n",
    "dataset = \"classified_data\"\n",
    "# create IoT Analytics client\n",
    "iota_client = boto3.client('iotanalytics')\n",
    "\n",
    "# import target Data Set from AWS IoT Analytics service\n",
    "try:\n",
    "    dataset_url = iota_client.get_dataset_content(datasetName = dataset)['entries'][0]['dataURI']\n",
    "    data_all = pd.read_csv(dataset_url)\n",
    "    if data_all.empty:\n",
    "        raise Exception('No data found in AWS IoT Analytics data set - can you please ensure you have data and clicked Actions->Run Now in the data set? ')\n",
    "    # iot analytics stores all column names as lowercase\n",
    "    VIEWING_ATTRIBUTES = [x.lower() for x in VIEWING_ATTRIBUTES]\n",
    "    TRAIN_ATTRIBUTES = [x.lower() for x in TRAIN_ATTRIBUTES]\n",
    "    OBSERVATION_FEATURES = [x.lower() for x in OBSERVATION_FEATURES]\n",
    "    CSV_HEADERS = [x.lower() for x in CSV_HEADERS]\n",
    "    dd_all = data_all[[x.lower() for x in VIEWING_ATTRIBUTES]]\n",
    "    dd_features = data_all[[x.lower() for x in TRAIN_ATTRIBUTES]]\n",
    "    dd_observation = data_all[[x.lower() for x in OBSERVATION_FEATURES]]\n",
    "    #display(data_all)\n",
    "    logger.info(\"loaded data from AWS IoT Analytics dataset '%s'\", dataset)\n",
    "    if data_all.empty:\n",
    "        raise Exception('No data found')\n",
    "# use backup dataset if dataset not found\n",
    "except Exception as e:\n",
    "    logger.error(\"Cannot load dataset '%s' from AWS IoT Analytics - use generic csv file \", dataset)\n",
    "    logger.error(e)\n",
    "    data_all = pd.read_csv('https://s3-eu-west-1.amazonaws.com/industrial-architecture-workshop-eu-west-1/2020-06-03/sagemaker/Drill-Data-All.csv')\n",
    "    dd_all = data_all[VIEWING_ATTRIBUTES]\n",
    "    dd_features = data_all[TRAIN_ATTRIBUTES]\n",
    "    dd_observation = data_all[OBSERVATION_FEATURES]\n",
    "finally:\n",
    "    logger.info(\"Shape of complete matrix: \" + str(np.shape(dd_all)))\n",
    "    logger.info(\"Shape of feature matrix: \" + str(np.shape(dd_features)))\n",
    "    logger.info(\"Shape of observation: \" + str(np.shape(dd_observation)))\n",
    "    #display(dd_all) # uncomment this line to see the data in detail (optional)\n",
    "    dd_all_description = dd_all.describe() # store the data description\n",
    "    dd_all.describe() # print the description of the data for viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data set\n",
    "We have to normalize the attributes to [0,1] range to avoid numerical problems during the learning process. Note that the normalization is only done to the features, but not to the observations. The observations are already 0,1-values.\n",
    "\n",
    "Note that this step can take a while depending on the performance of the underlying hardware! The code has been written with readibility in mind and therefore does not use a vectorized implementation, but instead a more easily understandable \"looped\" implementation. For larger data sets, it is highly recommended to use a vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress a confusing warning about writing to a data frame.\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def normalizeFeatureValue(value, min_val, min_max_diff):\n",
    "    return (value - min_val) / min_max_diff\n",
    "\n",
    "def normalizeAttribute(inputData, description, min_val=None, max_val=None):\n",
    "    if (min_val is None):\n",
    "        min_val = float(description[\"min\"])\n",
    "    if (max_val is None):\n",
    "        max_val = float(description[\"max\"])\n",
    "    diff = max_val - min_val\n",
    "    for index in range(0,len(inputData)):\n",
    "        inputData[index] = normalizeFeatureValue(value=inputData[index], min_val=min_val, min_max_diff=diff)\n",
    "    return inputData\n",
    "\n",
    "def normalize(data, attributes=TRAIN_ATTRIBUTES, description=None):\n",
    "    if (description is None):\n",
    "        description = data.describe()\n",
    "    # make a deep copy to avoid reference bugs\n",
    "    resultData = copy.deepcopy(data)\n",
    "    # normalize by attribute\n",
    "    for attr in attributes:\n",
    "        #logger.info(\"\\n**********\\nNormalizing input data of attribute \" + str(attr) + \"\\n**********\")\n",
    "        normalizeAttribute(resultData[attr], description[attr])\n",
    "    \n",
    "    return resultData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run these normalization functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Normalization took 3.13994598389 seconds.\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "dd_all_norm = normalize(dd_all)\n",
    "endTime = time.time()\n",
    "dd_all_norm.describe()\n",
    "\n",
    "loopedTime = endTime - startTime\n",
    "logger.info(\"Normalization took \" + str(loopedTime) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL: Vectorization\n",
    "The following step is ***optional*** and uses a vectorized implementation of the normalization step. There is only a single loop over the different attributes. Contrary to the looped version of the normalization, this has only a single line. Looking carefully at the data types, the line has a mixed set of operands: While `resultData[attr]` is a vector containing all the values of the current attribute `attr`, the other operands are scalars (floats). The vectorized and highly optimized operators that allow the subtraction/division of a vector by a scalar are the foundation of the speed improvement that we will be able to see when executing this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Vectorized version took 0.0219140052795 sec. compared to 3.13994598389 sec. with loops.\n",
      "INFO:__main__:Vectorization has an improvement of 14328.4896751% over the loop implementation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hasanomaly</th>\n",
       "      <th>avgpressure</th>\n",
       "      <th>avgmotor</th>\n",
       "      <th>avgspindle</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7364.000000</td>\n",
       "      <td>7364.000000</td>\n",
       "      <td>7364.000000</td>\n",
       "      <td>7364.000000</td>\n",
       "      <td>7364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317219</td>\n",
       "      <td>0.213074</td>\n",
       "      <td>0.439226</td>\n",
       "      <td>0.538595</td>\n",
       "      <td>0.292958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.465425</td>\n",
       "      <td>0.223895</td>\n",
       "      <td>0.184287</td>\n",
       "      <td>0.176701</td>\n",
       "      <td>0.225032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159601</td>\n",
       "      <td>0.419988</td>\n",
       "      <td>0.531790</td>\n",
       "      <td>0.121045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161062</td>\n",
       "      <td>0.422786</td>\n",
       "      <td>0.534343</td>\n",
       "      <td>0.239796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162497</td>\n",
       "      <td>0.425598</td>\n",
       "      <td>0.536965</td>\n",
       "      <td>0.357423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hasanomaly  avgpressure     avgmotor   avgspindle     duration\n",
       "count  7364.000000  7364.000000  7364.000000  7364.000000  7364.000000\n",
       "mean      0.317219     0.213074     0.439226     0.538595     0.292958\n",
       "std       0.465425     0.223895     0.184287     0.176701     0.225032\n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "25%       0.000000     0.159601     0.419988     0.531790     0.121045\n",
       "50%       0.000000     0.161062     0.422786     0.534343     0.239796\n",
       "75%       1.000000     0.162497     0.425598     0.536965     0.357423\n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalizeVectorized(data, attributes=TRAIN_ATTRIBUTES, description=None):\n",
    "    if (description is None):\n",
    "        description = data.describe()\n",
    "    # make a deep copy to avoid reference bugs\n",
    "    resultData = copy.deepcopy(data)\n",
    "    # normalize by attribute\n",
    "    for attr in attributes:\n",
    "        min_val = float(description[attr][\"min\"])\n",
    "        max_val = float(description[attr][\"max\"])\n",
    "        \n",
    "        # vectorized normalization implementation\n",
    "        resultData[attr] = (resultData[attr] - min_val) / (max_val - min_val)\n",
    "    return resultData\n",
    "\n",
    "startTime = time.time()\n",
    "vectorizedResult = normalizeVectorized(dd_all)\n",
    "endTime = time.time()\n",
    "vectorizedTime = endTime - startTime\n",
    "\n",
    "logger.info(\"Vectorized version took \" + str(vectorizedTime) + \" sec. compared to \" + str(loopedTime) + \" sec. with loops.\")\n",
    "logger.info(\"Vectorization has an improvement of \" + str(loopedTime / vectorizedTime * 100) + \"% over the loop implementation\")\n",
    "\n",
    "vectorizedResult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the computation is exactly the same as in the looped implementation. A typical result of the execution is as follows:\n",
    ">`INFO:__main__:Vectorized version took 0.0220000743866 sec. compared to 0.638000011444 sec. with loops.\n",
    "INFO:__main__:Vectorization has an improvement of 2899.99024655% over the loop implementation`\n",
    "\n",
    "This means even for this small data set, the vectorized implementation was about 28-times faster than the looped implementation. This gap widens as soon as datasets grow bigger and fail to fit into main memory.\n",
    "\n",
    "***Learning***: If preprocessing is slow, check if everything has been vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test Data Sets\n",
    "As shown above, the data has now been normalized to values of the range [0,1]. Next, we split the data in the features X and the labels y and turn them into nd.array instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.15930896 0.42935792 0.5381709  0.19737233]\n",
       " [0.15955013 0.4175472  0.53268063 0.07995133]\n",
       " [0.1564667  0.4206642  0.53089744 0.27804887]\n",
       " ...\n",
       " [0.16155158 0.42147252 0.5355038  0.12083457]\n",
       " [0.9866026  0.00840182 0.53304225 0.9121569 ]\n",
       " [0.16119407 0.00765268 0.5329813  0.27758214]]\n",
       "<NDArray 7364x4 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.]\n",
       " [0.]\n",
       " [0.]\n",
       " ...\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]]\n",
       "<NDArray 7364x1 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = nd.array(dd_all_norm[TRAIN_ATTRIBUTES])\n",
    "y = nd.array(dd_all_norm[OBSERVATION_FEATURES])\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s measure the shapes of the two matrices X and y and store some information about them so we can use it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Each dataset in X has 4 features and there are 7364 samples.\n"
     ]
    }
   ],
   "source": [
    "X_shape = X.shape\n",
    "y_shape = y.shape\n",
    "totalSampleCount = X_shape[0]\n",
    "inputFeatureCount = X_shape[1]\n",
    "logger.info(\"Each dataset in X has \" + str(inputFeatureCount) + \" features and there are \" + str(totalSampleCount) + \" samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the training and the test data set. The training set will be used to train the model. The test data set will be used to determine how well the model generalizes over previously unseen data.\n",
    "\n",
    "We use 80% of the samples for training and 20% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Expecting the training set to have 5891 samples.\n",
      "INFO:__main__:Expecting the test set to have 1473 samples.\n",
      "INFO:__main__:Given that the total number of samples is 7364, this is expected.\n",
      "INFO:__main__:The X portion of the sample is:\n",
      "\n",
      "[0.9790082  0.98531234 0.5332652  0.27802882]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "\n",
      "INFO:__main__:The y portion of the sample is:\n",
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "num_outputs = 2\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "\n",
    "trainSampleCount = int(totalSampleCount*train_test_ratio)\n",
    "testSampleCount = int(totalSampleCount - trainSampleCount)\n",
    "logger.info(\"Expecting the training set to have \" + str(trainSampleCount) + \" samples.\")\n",
    "logger.info(\"Expecting the test set to have \" + str(testSampleCount) + \" samples.\")\n",
    "logger.info(\"Given that the total number of samples is \" + str(totalSampleCount) + \", this is expected.\")\n",
    "\n",
    "train_dset = mx.gluon.data.dataset.ArrayDataset(X[:trainSampleCount, :], y[:trainSampleCount])\n",
    "test_dset = mx.gluon.data.dataset.ArrayDataset(X[trainSampleCount:, :], y[trainSampleCount:])\n",
    "\n",
    "# get a sample from the dataset\n",
    "index = 5\n",
    "sample_X = train_dset[index][0] # get item at index 5 and 0 => 0 because X is the first part of the data set\n",
    "sample_y = train_dset[index][1] # get item at index 5 and 1 => 0 because y is the first part of the data set\n",
    "logger.info(\"The X portion of the sample is:\\n\" + str(sample_X) + \"\\n\\n\")\n",
    "logger.info(\"The y portion of the sample is:\\n\" + str(sample_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "In this section, we create two data loaders: One contains the training data and the other one contains the test data. The third one created here is just for viewing purposes to see what kind of data will be produced by the loader.\n",
    "The loader will provide always two pieces - features X (X_batch) and a corresponding label (y_batch) -  and each batch contains 64 samples (as defined by batch_size). The shapes printed by the for loop at the end of this piece of code should reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch has shape (64, 4), and y_batch has shape (64, 1)\n",
      "X_batch has shape (64, 4), and y_batch has shape (64, 1)\n",
      "X_batch has shape (64, 4), and y_batch has shape (64, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # usually defined so that one batch fits into memory. Not really relevant for this example.\n",
    "\n",
    "# create a data loader for the sets\n",
    "train_loader = mx.gluon.data.DataLoader(train_dset, batch_size=batch_size)\n",
    "test_loader = mx.gluon.data.DataLoader(test_dset, batch_size=batch_size)\n",
    "show_loader = mx.gluon.data.DataLoader(train_dset, batch_size=batch_size)\n",
    "\n",
    "# show how the loader works\n",
    "count = 0\n",
    "for X_batch, y_batch in show_loader:\n",
    "    print(\"X_batch has shape {}, and y_batch has shape {}\".format(X_batch.shape, y_batch.shape))\n",
    "    count += 1\n",
    "    if (count > 2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Neural Network\n",
    "\n",
    "For this example, we will use a \"very boring\" neural network (NN) architecture. It will just chain 3 layers together and use a relu activation function. All nodes in each layer are connected to all nodes in the following layer (dense network).\n",
    "Given the drill data characteristics, this network should be sufficient to make accurate predictions whether a given data set is an anomaly or not.\n",
    "\n",
    "Note that adding more layers or using other activation functions is rather simple due to Gluon's `Sequential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 4\n",
    "# this function returns a neural network based on the corresponding gluon sequential class\n",
    "# use gluon.nn.Sequential() to build a normal sequential model\n",
    "# use gluon.nn.HybridSequential() to build a hybrid model. The hybrid model can be saved along with the parameters\n",
    "def buildNeuralNetwork(net=gluon.nn.Sequential()):\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(num_outputs))\n",
    "    return net\n",
    "\n",
    "def buildHybridNeuralNetwork():\n",
    "    net = buildNeuralNetwork(net=gluon.nn.HybridSequential())\n",
    "    net.hybridize()\n",
    "    return net\n",
    "\n",
    "net = buildHybridNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "Based on this network definition, we have to provide initial values for the parameters of the NN. Gluon also has a nice set of functions and classes that takes care of that. Basically, this line assigns random values drawn from a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx, force_reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross-entropy loss\n",
    "It is mandatory for the optimizer to have a loss function to determine how \"far\" away from the correct predictions it was. Again, Gluon has a set of these loss functions already implemented, so we can just use one. In this case, SoftmaxCrossEntropyLoss is the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "This line defines a trainer - basically Gluons terminology for the class that runs the training. The optimizer we use here is called \"Adam\", but we an also use Single Gradient Descent (abbreviated \"sgd\") as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate})\n",
    "#trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric\n",
    "Next, we define a functiont that will use the NN and some data loader to determine the accuracy. This will allow us to determine the accuracy on the train as well as the test set by simply passing this function a different loader. The metric to compute the accuracy is provided as a class by Gluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy() # get the Gluon metric to compute accuracy for classification\n",
    "    for data, label in data_iterator: # use the data in the data loader to run them through the evaluation\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, inputFeatureCount)) # here we load the features\n",
    "        label = label.as_in_context(model_ctx) # here we get the corresponding labels\n",
    "        output = net(data) # this is where we run the features through the network to make predictions\n",
    "        '''\n",
    "        The next line is a bit more complicated so we explain it in detail here:\n",
    "        The output of the NN are two values for each input dataset - the first is the probability that the dataset belongs to\n",
    "        the first class and the second one is the probability that it belongs to the second class. \n",
    "        '''\n",
    "        predictions = nd.argmax(output, axis=1).reshape((-1, 1))\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "This is the main loop that runs through the training data to train the model. At the end of each epoch (once it has run through the training data once), it will evaluate the model by measuring the accuracy on the test and the train set.\n",
    "The accuracy of the model on the test set is what we are looking for, because the higher this accuracy is, the better is the model. This is because the test data contains previously unseen data, i.e., the prediction is made on data that the model has not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3980.340631008148, Train_acc 0.683754880326, Test_acc 0.678886625933\n",
      "Epoch 10. Loss: 1487.996562719345, Train_acc 0.934306569343, Test_acc 0.928716904277\n",
      "Epoch 20. Loss: 622.5797629356384, Train_acc 0.97657443558, Test_acc 0.974202308215\n",
      "Epoch 30. Loss: 425.7857804298401, Train_acc 0.98081819725, Test_acc 0.978954514596\n",
      "Epoch 40. Loss: 242.20052921772003, Train_acc 0.984382957053, Test_acc 0.983706720978\n",
      "Epoch 50. Loss: 87.65450006723404, Train_acc 0.999320998133, Test_acc 1.0\n",
      "Epoch 60. Loss: 28.776299942284822, Train_acc 1.0, Test_acc 1.0\n",
      "Epoch 70. Loss: 12.073033042252064, Train_acc 1.0, Test_acc 1.0\n",
      "Epoch 80. Loss: 5.882756280712783, Train_acc 1.0, Test_acc 1.0\n",
      "Epoch 90. Loss: 3.103112092707306, Train_acc 1.0, Test_acc 1.0\n",
      "Epoch 100. Loss: 1.7146259879227728, Train_acc 1.0, Test_acc 1.0\n",
      "Epoch 110. Loss: 0.9753940722439438, Train_acc 1.0, Test_acc 1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for (data, label) in train_loader:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, inputFeatureCount))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_loader, net)\n",
    "    train_accuracy = evaluate_accuracy(train_loader, net)\n",
    "    if (e % 10 == 0):\n",
    "        print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "              (e, cumulative_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this code has finished executing, the training of the NN has finished. The two last lines of output should look similar to the following:\n",
    ">`\n",
    "Epoch 100. Loss: 235.31822562217712, Train_acc 0.958559782609, Test_acc 0.953929539295\n",
    "Epoch 110. Loss: 190.30429339408875, Train_acc 0.968070652174, Test_acc 0.956639566396\n",
    "`\n",
    "\n",
    "This means the accuracy over the training set, i.e., data that has been used for training, is almost 97%. Given the simplicity of the NN and the small size of the data set, this is already a good result. To check how the model performs with previously unseen data, i.e., data that has not been used for training, the evaluation also uses the training set. The output above means that our model achieves about 96% accuracy on this unknown data set.\n",
    "\n",
    "To avoid lengthy training & wait times, the training has been limited to 120 epochs, i.e., the training loop is executed 120 times. Looking at the loss shown in the output, we can see that it was still declining after the 120th epoch. This can indicate that continuing the training may yield better results. For the purposes of this notebook, results similar to the ones above are sufficient. Hence, it is ***optional*** to re-execute the training loop again and see if the accuracy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "This section shows how to use the model to make predictions using some artifical examples. This allows us to check whether the model will find anomalies that we expect to be detected.\n",
    "\n",
    "## An \"Average\" Drilling procedure\n",
    "First, let´s define an artificial piece of data using the mean values of the input data that we had. The following code creates this artificial data set and also collects the min/max values of the corresponding attributes. These values are needed to normalize the input later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min-values are: [7.815327713140493, 849.0821682136137, 551.0287156231697, 10.01059556007385]\n",
      "The max-values are: [24.4976786018849, 2223.4170737832915, 2158.418250804427, 22.64933466911316]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.369910665017434,\n",
       " 1452.7263457731235,\n",
       " 1416.7602756228212,\n",
       " 13.713214136089467]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = []\n",
    "min_values = []\n",
    "max_values = []\n",
    "for attr in TRAIN_ATTRIBUTES:\n",
    "    features.append(dd_all_description[attr][\"mean\"])\n",
    "    min_values.append(dd_all_description[attr][\"min\"])\n",
    "    max_values.append(dd_all_description[attr][\"max\"])\n",
    "\n",
    "print(\"The min-values are: \" + str(min_values))    \n",
    "print(\"The max-values are: \" + str(max_values))\n",
    "\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the loop has collected the attribute values as a list and we can use this list of artificial attribute values as an input for our prediction. Note that these values are not normalized yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[  11.36991  1452.7263   1416.7603     13.713214]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-20035.268  19389.29 ]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avgData = nd.array(features)\n",
    "display(avgData)\n",
    "output = net(avgData.reshape((-1, inputFeatureCount)))\n",
    "display(output)\n",
    "predictions = nd.argmax(output, axis=1)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above has three parts: The first part is the result of the transformation of the list into an nd-array. The values have remained the same. The second part is the output of the NN which has been generated when the artificial data has been passed into the NN. In one of our executions, this yielded the following results:\n",
    ">`\n",
    "[[-12494.348  12190.844]]\n",
    "<NDArray 1x2 @cpu(0)>\n",
    "`\n",
    "\n",
    "The actual values may look different. Normally, the values are probabilities returned by the NN which indicate whether the input data corresponds to a normal drilling process or a non-normal one. The reason why these two values are not probabilities and hence do not yield a proper result is the missing normalization of the input.\n",
    "\n",
    "The code below changes this: The input generated above is normalized using the same function we have applied to the input data. Once this is done, we pass the normalized data into the NN again and the result is as expected: A set of average sensor values is labelled as a \"normal\" drilling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.21307445 0.4392264  0.5385948  0.29295793]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 5.7097406 -5.175905 ]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index in range(0, inputFeatureCount):\n",
    "    diff = max_values[index] - min_values[index]\n",
    "    avgData[index] = normalizeFeatureValue(avgData[index], min_val=min_values[index], min_max_diff=diff)\n",
    "display(avgData)\n",
    "\n",
    "output = net(avgData.reshape((-1, inputFeatureCount)))\n",
    "display(output)\n",
    "predictions = nd.argmax(output, axis=1)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s have a closer look at the result and interpret it a bit. Our execution yielded a result as follows:\n",
    ">`\n",
    "[[ 1.7810409 -1.7810413]]\n",
    "<NDArray 1x2 @cpu(0)>\n",
    "[0.]\n",
    "<NDArray 1 @cpu(0)>\n",
    "`\n",
    "\n",
    "This means the NN predicted that the input corresponds to a normal drilling process. The next piece of code wraps the whole procedure of normalizing the input and making a prediction on it into a set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Numerical prediction result: \n",
      "[0.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "INFO:__main__:Boolean prediction result: False\n"
     ]
    }
   ],
   "source": [
    "# Note: the following two functions can also be used to make predictions on multiple input data set\n",
    "# Therefore, both return arrays, more specifically, their return values are instances of nd.array\n",
    "def __makePrediction(normalizedInputData, net=net):\n",
    "    output = net(normalizedInputData.reshape((-1, inputFeatureCount)))\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "def makePrediction(rawInputData, minValues=min_values, maxValues=max_values, net=net):\n",
    "    if (len(rawInputData) != inputFeatureCount):\n",
    "        raise IllegalArgumentException(\"Out model only has \" + str(inputFeatureCount) + \" inputs. You cannot use more.\")\n",
    "    data = nd.array(rawInputData) # make sure the input is nd.array\n",
    "    # normalize\n",
    "    for index in range(0, len(rawInputData)):\n",
    "        diff = max_values[index] - min_values[index]\n",
    "        data[index] = normalizeFeatureValue(data[index], min_val=min_values[index], min_max_diff=diff)\n",
    "    # make a prediction\n",
    "    return __makePrediction(normalizedInputData=data, net=net)\n",
    "\n",
    "# This method makes a prediction on a single set of sensor values and returns a single int value\n",
    "def predict(pressure, motor, spindle, duration, minValues=min_values, maxValues=max_values, net=net):\n",
    "    # generate a list of feature values\n",
    "    features = [pressure, motor, spindle, duration]\n",
    "    # make a prediction\n",
    "    r = makePrediction(rawInputData=features, minValues=minValues, maxValues=maxValues, net=net)\n",
    "    return int(r.asscalar())\n",
    "\n",
    "# This method makes a prediction on a single set of sensor values and returns TRUE if it is a normal dataset \n",
    "# and otherwise False.\n",
    "def isNormal(pressure, motor, spindle, duration, minValues=min_values, maxValues=max_values, net=net):\n",
    "    if (predict(pressure=pressure, motor=motor, spindle=spindle, duration=duration, minValues=minValues, maxValues=maxValues, net=net) == 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# make some example predictions\n",
    "logger.info(\"Numerical prediction result: \" + str(makePrediction(features)) + \"\\n\") # should return 0 as above\n",
    "logger.info(\"Boolean prediction result: \" + str(isNormal(1,2,3,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Exceptional Drilling Procedure\n",
    "Next, we create a sample that must be detected as an anomaly using the max values for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.4976786018849, 2223.4170737832915, 2158.418250804427, 22.64933466911316]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxFeatures = []\n",
    "for attr in TRAIN_ATTRIBUTES:\n",
    "    maxFeatures.append(dd_all_description[attr][\"max\"])\n",
    "    \n",
    "display(maxFeatures)\n",
    "\n",
    "makePrediction(rawInputData=maxFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing & Exporting the model\n",
    "To make use of the model in an IoT context, we have to export it and deploy it on edge devices. This requires the following steps:\n",
    "* Exporting the model and storing it to a file along with the min/max values required for normalization of the data\n",
    "* Uploading the files to S3\n",
    "* Deploy the model on edge devices\n",
    "This notebook only addresses the first two items of this list. The actual deployment on edge devices is outside of the scope of this notebook.\n",
    "\n",
    "## Export\n",
    "The export of the NN and all relevant parts for predictions requires 3 files: One file contains the structure of the network, i.e., a description of the layers and their structure. The second file contains the set of parameters that are used for the prediction and have been learned during the training phase. Finally, the third file has the variables needed to normalize input with the corresponding min-max-values that have been used for the normalization of the training data.\n",
    "\n",
    "First, we define a couple of names that will allow us easier access to the files later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix=\"Drill-AnomalyClassifier\"\n",
    "epoch = 0\n",
    "paramFile = filePrefix + \"-0000\" + \".params\"\n",
    "structureFile = filePrefix + \"-symbol.json\"\n",
    "normalizationFile = filePrefix + \"-normalization.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line uses Gluon functionality to write 1) the network structure and 2) the NN parameters derived during training. The network structure is stored as JSON in a file whose name is generated from the given prefix and \"-symbol.json\". The parameters are stored in the \".params\" file using the prefix as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line does the export of the parameters and the NN architecture/structure\n",
    "net.export(filePrefix, epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can load the file and have a look at the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading structure file with name Drill-AnomalyClassifier-symbol.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'arg_nodes': [0, 1, 2, 5, 6, 9, 10],\n",
       " u'attrs': {u'mxnet_version': [u'int', 10600]},\n",
       " u'heads': [[11, 0, 0]],\n",
       " u'node_row_ptr': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " u'nodes': [{u'inputs': [], u'name': u'data', u'op': u'null'},\n",
       "  {u'attrs': {u'__dtype__': u'0',\n",
       "    u'__lr_mult__': u'1.0',\n",
       "    u'__shape__': u'(4, 0)',\n",
       "    u'__storage_type__': u'0',\n",
       "    u'__wd_mult__': u'1.0'},\n",
       "   u'inputs': [],\n",
       "   u'name': u'hybridsequential3_dense0_weight',\n",
       "   u'op': u'null'},\n",
       "  {u'attrs': {u'__dtype__': u'0',\n",
       "    u'__init__': u'zeros',\n",
       "    u'__lr_mult__': u'1.0',\n",
       "    u'__shape__': u'(4,)',\n",
       "    u'__storage_type__': u'0',\n",
       "    u'__wd_mult__': u'1.0'},\n",
       "   u'inputs': [],\n",
       "   u'name': u'hybridsequential3_dense0_bias',\n",
       "   u'op': u'null'},\n",
       "  {u'attrs': {u'flatten': u'True', u'no_bias': u'False', u'num_hidden': u'4'},\n",
       "   u'inputs': [[0, 0, 0], [1, 0, 0], [2, 0, 0]],\n",
       "   u'name': u'hybridsequential3_dense0_fwd',\n",
       "   u'op': u'FullyConnected'},\n",
       "  {u'attrs': {u'act_type': u'relu'},\n",
       "   u'inputs': [[3, 0, 0]],\n",
       "   u'name': u'hybridsequential3_dense0_relu_fwd',\n",
       "   u'op': u'Activation'},\n",
       "  {u'attrs': {u'__dtype__': u'0',\n",
       "    u'__lr_mult__': u'1.0',\n",
       "    u'__shape__': u'(4, 0)',\n",
       "    u'__storage_type__': u'0',\n",
       "    u'__wd_mult__': u'1.0'},\n",
       "   u'inputs': [],\n",
       "   u'name': u'hybridsequential3_dense1_weight',\n",
       "   u'op': u'null'},\n",
       "  {u'attrs': {u'__dtype__': u'0',\n",
       "    u'__init__': u'zeros',\n",
       "    u'__lr_mult__': u'1.0',\n",
       "    u'__shape__': u'(4,)',\n",
       "    u'__storage_type__': u'0',\n",
       "    u'__wd_mult__': u'1.0'},\n",
       "   u'inputs': [],\n",
       "   u'name': u'hybridsequential3_dense1_bias',\n",
       "   u'op': u'null'},\n",
       "  {u'attrs': {u'flatten': u'True', u'no_bias': u'False', u'num_hidden': u'4'},\n",
       "   u'inputs': [[4, 0, 0], [5, 0, 0], [6, 0, 0]],\n",
       "   u'name': u'hybridsequential3_dense1_fwd',\n",
       "   u'op': u'FullyConnected'},\n",
       "  {u'attrs': {u'act_type': u'relu'},\n",
       "   u'inputs': [[7, 0, 0]],\n",
       "   u'name': u'hybridsequential3_dense1_relu_fwd',\n",
       "   u'op': u'Activation'},\n",
       "  {u'attrs': {u'__dtype__': u'0',\n",
       "    u'__lr_mult__': u'1.0',\n",
       "    u'__shape__': u'(2, 0)',\n",
       "    u'__storage_type__': u'0',\n",
       "    u'__wd_mult__': u'1.0'},\n",
       "   u'inputs': [],\n",
       "   u'name': u'hybridsequential3_dense2_weight',\n",
       "   u'op': u'null'},\n",
       "  {u'attrs': {u'__dtype__': u'0',\n",
       "    u'__init__': u'zeros',\n",
       "    u'__lr_mult__': u'1.0',\n",
       "    u'__shape__': u'(2,)',\n",
       "    u'__storage_type__': u'0',\n",
       "    u'__wd_mult__': u'1.0'},\n",
       "   u'inputs': [],\n",
       "   u'name': u'hybridsequential3_dense2_bias',\n",
       "   u'op': u'null'},\n",
       "  {u'attrs': {u'flatten': u'True', u'no_bias': u'False', u'num_hidden': u'2'},\n",
       "   u'inputs': [[8, 0, 0], [9, 0, 0], [10, 0, 0]],\n",
       "   u'name': u'hybridsequential3_dense2_fwd',\n",
       "   u'op': u'FullyConnected'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Loading structure file with name \" + str(structureFile))\n",
    "with open(structureFile) as f:\n",
    "    nn_json = json.load(f)\n",
    "display(nn_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we create a dictionary that contains the variables needed to do normalization of input data. Using the standard methods for writing a JSON file, can store them on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAttributeKey = \"train-attributes\"\n",
    "observationKey = \"observation-attribute\"\n",
    "minValuesKey = \"min-values\"\n",
    "maxValuesKey = \"max-values\"\n",
    "\n",
    "normalizationDict = {}\n",
    "normalizationDict[trainAttributeKey] = TRAIN_ATTRIBUTES\n",
    "normalizationDict[observationKey] = OBSERVATION_FEATURES\n",
    "normalizationDict[minValuesKey] = min_values\n",
    "normalizationDict[maxValuesKey] = max_values\n",
    "\n",
    "with open(normalizationFile, \"w\") as f:\n",
    "    json.dump(normalizationDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model from file\n",
    "This section demonstrates what needs to happen on the device to reload the model and get a neural network back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded Training attrs:\t[u'avgpressure', u'avgmotor', u'avgspindle', u'duration']\n",
      "INFO:__main__:Loaded Observation attr:\t[u'hasanomaly']\n",
      "INFO:__main__:Loaded Min-Values:\t[7.815327713140493, 849.0821682136137, 551.0287156231697, 10.01059556007385]\n",
      "INFO:__main__:Loaded Max-Values:\t[24.4976786018849, 2223.4170737832915, 2158.418250804427, 22.64933466911316]\n"
     ]
    }
   ],
   "source": [
    "with open(normalizationFile) as f:\n",
    "    newNormalizationDict = json.load(f)\n",
    "newTrainAttributes = newNormalizationDict[trainAttributeKey]\n",
    "newObservationAttribute = newNormalizationDict[observationKey]\n",
    "newMinValues = newNormalizationDict[minValuesKey]\n",
    "newMaxValues = newNormalizationDict[maxValuesKey]\n",
    "\n",
    "logger.info(\"Loaded Training attrs:\\t\" + str(newTrainAttributes))\n",
    "logger.info(\"Loaded Observation attr:\\t\" + str(newObservationAttribute))\n",
    "logger.info(\"Loaded Min-Values:\\t\" + str(newMinValues))\n",
    "logger.info(\"Loaded Max-Values:\\t\" + str(newMaxValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deserialized_net = gluon.nn.SymbolBlock.imports(structureFile, ['data'], paramFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with a loaded model\n",
    "Finally, we can use the model as we did before. Let´s re-play the two artificial examples from above, but instead of the original NN, we now use the deserialized NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__makePrediction(normalizedInputData=avgData, net=deserialized_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makePrediction(rawInputData=maxFeatures, net=deserialized_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the first example returns a 0 (normal) prediction, because it used the averages for the sensor input which are within the normal bounds of the drilling operation. The second example uses extreme values and therefore the model predicts an anomaly.\n",
    "\n",
    "To show that we can still normalize values, let´s create a new, artificial dataset, normalize it and then make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pressure = 7.815328, # edit here if you want\n",
    "motor = 849.082168, # edit here if you want\n",
    "spindle = 551.028716, # edit here if you want\n",
    "duration = 10.010596, # edit here if you want\n",
    "\n",
    "isNormal(pressure=pressure, # this is the arbitrary pressure value from above\n",
    "         motor=motor, # this is the arbitrary motor rpm value from above\n",
    "         spindle=spindle, # this is the arbitrary spindle rpm value from above\n",
    "         duration=duration, # this is the arbitrary duration value from above\n",
    "         minValues=newMinValues, # the loaded min values\n",
    "         maxValues=newMaxValues, # the loaded max values\n",
    "         net=deserialized_net) # the deserialized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the model in S3\n",
    "To make use of the model in Greengrass, we need to store it as a file in S3. The following code will compress the three parts into a single file and then push that to a S3 bucket.\n",
    "***Important*** This part will fail to run on your machine if the following conditions are not met:\n",
    "1. Editing the code and replacing `iiotws3-iotwss3bucket-btgokikhcqx8` with the name of the bucket in the account you are using. (If you booted this Notebook through th workshop CloudFormation template this is already done for you)\n",
    "2. If you are running this from Cloud9 or another EC2 instance, this resource needs to have a role that allows writing to the aforementioned S3 bucket.\n",
    "3. If you are running this from a local PC, you need to have access key credentials installed that allow writing to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:load model to s3://iiotws-iiotwsgreengrass-yu8stn8r1q3p/model/DrillingPrediction.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "store_model_on_s3 = True\n",
    "compressed_model_file_name = \"DrillingPrediction.model.tar.gz\"\n",
    "amazon_s3_bucket_for_model_storage = \"iiotws-iiotwsgreengrass-yu8stn8r1q3p\"\n",
    "amazon_s3_key_for_compressed_model = \"model/\"+compressed_model_file_name\n",
    "\n",
    "if store_model_on_s3:\n",
    "    with tarfile.open(compressed_model_file_name, \"w:gz\") as tar:\n",
    "        tar.add(paramFile, arcname=os.path.basename(paramFile))\n",
    "        tar.add(structureFile, arcname=os.path.basename(structureFile))\n",
    "        tar.add(normalizationFile, arcname=os.path.basename(normalizationFile))\n",
    "    s3 = boto3.resource('s3')\n",
    "    logger.info(\"load model to s3://{}/{}\".format(amazon_s3_bucket_for_model_storage, amazon_s3_key_for_compressed_model))\n",
    "    s3.Bucket(amazon_s3_bucket_for_model_storage).upload_file(compressed_model_file_name, amazon_s3_key_for_compressed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
