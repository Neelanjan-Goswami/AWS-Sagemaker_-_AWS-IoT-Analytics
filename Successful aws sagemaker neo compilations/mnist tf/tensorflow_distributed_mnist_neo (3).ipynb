{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow BYOM: Train with Custom Training Script, Compile with Neo, and Deploy on SageMaker\n",
    "\n",
    "This notebook can be compared to [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) in terms of its functionality. We will do the same classification task, but this time we will compile the trained model using the Neo API backend, to optimize for our choice of hardware. Finally, we setup a real-time hosted endpoint in SageMaker for our compiled model using the Neo Deep Learning Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f3159960278>\n",
      "int64_list {\n",
      "  value: 28\n",
      "}\n",
      "\n",
      "bytes_list {\n",
      "  value: \"\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000a`Mv=\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000Z\\212\\353\\353\\353\\353\\353\\353\\373\\373\\370\\376\\365\\353\\276\\025\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\214\\373\\376\\376\\376\\376\\376\\376\\376\\376\\376\\376\\376\\376\\376\\376\\275\\027\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\342\\376\\320\\307\\307\\307\\307\\213=====\\200\\336\\376\\376\\275\\025\\000\\000\\000\\000\\000\\000\\000\\000\\000&R\\r\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\\"\\325\\376\\376s\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000T\\376\\376\\352\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000T\\376\\376\\352\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000j\\235\\376\\376\\3633\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\031u\\344\\344\\344\\375\\376\\376\\376\\376\\360\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000Dw\\334\\376\\376\\376\\376\\376\\376\\376\\376\\376\\216\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000%\\273\\375\\376\\376\\376\\337\\316\\316KD\\327\\376\\376u\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000q\\333\\376\\362\\343sY\\037\\000\\000\\000\\000\\310\\376\\361)\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\251\\376\\260>\\000\\000\\000\\000\\000\\000\\0000\\347\\376\\352\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\022|\\000\\000\\000\\000\\000\\000\\000\\000\\000T\\376\\376\\246\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\213\\376\\3569\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\322\\372\\376\\250\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\362\\376\\3579\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000Y\\373\\361V\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\005\\316\\366\\235\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\004uE\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "}\n",
      "\n",
      "p 7\n",
      "Writing data/train.tfrecords\n",
      "Writing data/validation.tfrecords\n",
      "Writing data/test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "print(data_sets.test)\n",
    "\n",
    "x = data_sets.train.images[0].shape[0]\n",
    "y= tf.train.Feature(int64_list=tf.train.Int64List(value=[x]))\n",
    "print(y)\n",
    "\n",
    "z = data_sets.train.images[0]\n",
    "a_raw = z.tostring()\n",
    "\n",
    "b= tf.train.Feature(bytes_list=tf.train.BytesList(value=[a_raw]))\n",
    "print(b)\n",
    "print('p',p)\n",
    "p = data_sets.train.labels[0]\n",
    "\n",
    "newdir = os.path.join('data', 'trial' + '.tfrecords')\n",
    "writernew = tf.python_io.TFRecordWriter(newdir)\n",
    "\n",
    "newexample = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'somefeature': y,\n",
    "            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[p])),\n",
    "            'a_raw': b}))\n",
    "writernew.write(newexample.SerializeToString())\n",
    "writernew.close()\n",
    "\n",
    "\n",
    "#print(data_sets.train.images[0][1])\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for distributed training \n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = 'inputs'\r\n",
      "SIGNATURE_NAME = 'predictions'\r\n",
      "\r\n",
      "LEARNING_RATE = 0.001\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(features, labels, mode, params):\r\n",
      "    # Input Layer\r\n",
      "    print(\"===>\",features, labels)\r\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\r\n",
      "\r\n",
      "    # Convolutional Layer #1\r\n",
      "    conv1 = tf.layers.conv2d(\r\n",
      "        inputs=input_layer,\r\n",
      "        filters=32,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "\r\n",
      "    # Pooling Layer #1\r\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\r\n",
      "    conv2 = tf.layers.conv2d(\r\n",
      "        inputs=pool1,\r\n",
      "        filters=64,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Dense Layer\r\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n",
      "    dropout = tf.layers.dropout(\r\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\r\n",
      "\r\n",
      "    # Logits Layer\r\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\r\n",
      "\r\n",
      "    # Define operations\r\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\r\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\r\n",
      "\r\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\r\n",
      "        global_step = tf.train.get_or_create_global_step()\r\n",
      "        label_indices = tf.cast(labels, tf.int32)\r\n",
      "        loss = tf.losses.softmax_cross_entropy(\r\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\r\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\r\n",
      "\r\n",
      "    if mode == Modes.PREDICT:\r\n",
      "        predictions = {\r\n",
      "            'probabilities': probabilities\r\n",
      "        }\r\n",
      "        export_outputs = {\r\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\r\n",
      "\r\n",
      "    if mode == Modes.TRAIN:\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\r\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n",
      "\r\n",
      "    if mode == Modes.EVAL:\r\n",
      "        eval_metric_ops = {\r\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, tf.argmax(input=logits, axis=1))\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "def serving_input_fn(params):\r\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [1, 784])}\r\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "def read_and_decode(filename_queue):\r\n",
      "    reader = tf.TFRecordReader()\r\n",
      "    _, serialized_example = reader.read(filename_queue)\r\n",
      "\r\n",
      "    features = tf.parse_single_example(\r\n",
      "        serialized_example,\r\n",
      "        features={\r\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\r\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\r\n",
      "        })\r\n",
      "\r\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n",
      "    image.set_shape([784])\r\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\r\n",
      "    label = tf.cast(features['label'], tf.int32)\r\n",
      "\r\n",
      "    return image, label\r\n",
      "\r\n",
      "\r\n",
      "def train_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def eval_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\r\n",
      "    test_file = os.path.join(training_dir, training_filename)\r\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\r\n",
      "\r\n",
      "    image, label = read_and_decode(filename_queue)\r\n",
      "    images, labels = tf.train.batch(\r\n",
      "        [image, label], batch_size=batch_size,\r\n",
      "        capacity=1000 + 3 * batch_size)\r\n",
      "\r\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\r\n",
      "\r\n",
      "def neo_preprocess(payload, content_type):\r\n",
      "    import logging\r\n",
      "    import numpy as np\r\n",
      "    import io\r\n",
      "\r\n",
      "    logging.info('Invoking user-defined pre-processing function')\r\n",
      "\r\n",
      "    if content_type != 'application/x-image' and content_type != 'application/vnd+python.numpy+binary':\r\n",
      "        raise RuntimeError('Content type must be application/x-image or application/vnd+python.numpy+binary')\r\n",
      "\r\n",
      "    f = io.BytesIO(payload)\r\n",
      "    image = np.load(f)*255\r\n",
      "\r\n",
      "    return image\r\n",
      "\r\n",
      "### NOTE: this function cannot use MXNet\r\n",
      "def neo_postprocess(result):\r\n",
      "    import logging\r\n",
      "    import numpy as np\r\n",
      "    import json\r\n",
      "\r\n",
      "    logging.info('Invoking user-defined post-processing function')\r\n",
      "\r\n",
      "    # Softmax (assumes batch size 1)\r\n",
      "    result = np.squeeze(result)\r\n",
      "    result_exp = np.exp(result - np.max(result))\r\n",
      "    result = result_exp / np.sum(result_exp)\r\n",
      "\r\n",
      "    response_body = json.dumps(result.tolist())\r\n",
      "    content_type = 'application/json'\r\n",
      "\r\n",
      "    return response_body, content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script here is and adaptation of the [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/mnist). It provides a ```model_fn(features, labels, mode)```, which is used for training, evaluation and inference. See [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) for more details about the training script.\n",
    "\n",
    "At the end of the training script, there are two additional functions, to be used with Neo Deep Learning Runtime:\n",
    "* `neo_preprocess(payload, content_type)`: Function that takes in the payload and Content-Type of each incoming request and returns a NumPy array\n",
    "* `neo_postprocess(result)`: Function that takes the prediction results produced by Deep Learining Runtime and returns the response body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.1.0 is the latest version of tensorflow that supports Python 2. Newer versions of tensorflow will only be available for Python 3.Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-05 14:03:35 Starting - Starting the training job...\n",
      "2020-10-05 14:03:37 Starting - Launching requested ML instances......\n",
      "2020-10-05 14:04:55 Starting - Preparing the instances for training......\n",
      "2020-10-05 14:05:51 Downloading - Downloading input data...\n",
      "2020-10-05 14:06:26 Training - Training image download completed. Training in progress.\u001b[34m2020-10-05 14:06:26,061 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:26,061 INFO - root - starting train task\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:26,075 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[34mDownloading s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,798 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,798 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,798 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,798 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,799 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,799 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'algo-1:2223', u'algo-2:2223'], u'worker': [u'algo-2:2222'], u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,799 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,801 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'master', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe2298058d0>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 2, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[34mdevice_filters: \"/job:master\"\u001b[0m\n",
      "\u001b[34mallow_soft_placement: true\u001b[0m\n",
      "\u001b[34mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints', '_master': u'grpc://algo-1:2222'}\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28,802 INFO - tensorflow - Start Tensorflow server.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28.949464: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:28.950349: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29,088 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29,089 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29,113 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[34m('===>', {'inputs': <tf.Tensor 'batch:0' shape=(100, 784) dtype=float32>}, <tf.Tensor 'batch:1' shape=(100,) dtype=int32>)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29,421 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29,422 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29.433207: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29.433239: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29.449612: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29.449641: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29.469484: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29.469550: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:29,993 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:30.000575: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:30.000605: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:31,461 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:31,467 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:31,494 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:31.568721: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:31.568766: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:32,141 INFO - tensorflow - Saving checkpoints for 0 into s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:26,619 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:26,619 INFO - root - starting train task\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:26,633 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[35mDownloading s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,862 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,862 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,862 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,862 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,862 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,862 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'algo-1:2223', u'algo-2:2223'], u'worker': [u'algo-2:2222'], u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'worker'}}\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,863 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,863 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd7771c58d0>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 2, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[35mdevice_filters: \"/job:worker/task:0\"\u001b[0m\n",
      "\u001b[35mallow_soft_placement: true\u001b[0m\n",
      "\u001b[35mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m, '_global_id_in_cluster': 1, '_is_chief': False, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints', '_master': u'grpc://algo-2:2222'}\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,867 INFO - tensorflow - Start Tensorflow server.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:30,897 INFO - tensorflow - Waiting 5 secs before starting training.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:34.721325: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:34.721374: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:06:35,413 INFO - tensorflow - loss = 2.3025348, step = 0\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,275 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[35mInstructions for updating:\u001b[0m\n",
      "\u001b[35mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,278 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[35mInstructions for updating:\u001b[0m\n",
      "\u001b[35mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,313 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[35m('===>', {'inputs': <tf.Tensor 'batch:0' shape=(100, 784) dtype=float32>}, <tf.Tensor 'batch:1' shape=(100,) dtype=int32>)\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,685 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,686 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,833 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,930 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,939 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:36,985 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[35mInstructions for updating:\u001b[0m\n",
      "\u001b[35mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[35m2020-10-05 14:06:37,672 INFO - tensorflow - loss = 1.0088942, step = 7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-05 14:06:50,089 INFO - tensorflow - global_step/sec: 7.01743\u001b[0m\n",
      "\u001b[35m2020-10-05 14:07:01,862 INFO - tensorflow - loss = 0.0615984, step = 190 (24.190 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:03,809 INFO - tensorflow - global_step/sec: 7.43396\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:04,869 INFO - tensorflow - loss = 0.05768078, step = 212 (29.457 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:17,044 INFO - tensorflow - global_step/sec: 7.63152\u001b[0m\n",
      "\u001b[35m2020-10-05 14:07:26,395 INFO - tensorflow - loss = 0.04406964, step = 374 (24.533 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:30,720 INFO - tensorflow - global_step/sec: 7.45856\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:33,856 INFO - tensorflow - loss = 0.05710558, step = 429 (28.987 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:44,442 INFO - tensorflow - global_step/sec: 7.5062\u001b[0m\n",
      "\u001b[35m2020-10-05 14:07:51,493 INFO - tensorflow - loss = 0.05259962, step = 561 (25.098 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:07:57,993 INFO - tensorflow - global_step/sec: 7.45303\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:02,526 INFO - tensorflow - loss = 0.056280788, step = 644 (28.669 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:11,755 INFO - tensorflow - global_step/sec: 7.3393\u001b[0m\n",
      "\u001b[35m2020-10-05 14:08:16,494 INFO - tensorflow - loss = 0.040906876, step = 748 (25.002 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:25,062 INFO - tensorflow - global_step/sec: 7.66471\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:31,498 INFO - tensorflow - loss = 0.013289449, step = 862 (28.973 sec)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:38,600 INFO - tensorflow - global_step/sec: 7.53468\u001b[0m\n",
      "\u001b[35m2020-10-05 14:08:40,316 INFO - tensorflow - loss = 0.01095522, step = 930 (23.822 sec)\u001b[0m\n",
      "\u001b[35m2020-10-05 14:08:50,103 INFO - tensorflow - Loss for final step: 0.008664884.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:50,049 INFO - tensorflow - Saving checkpoints for 1001 into s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:52.394635: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:52.394673: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:52,957 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[34m('===>', {'inputs': <tf.Tensor 'batch:0' shape=(100, 784) dtype=float32>}, <tf.Tensor 'batch:1' shape=(100,) dtype=int32>)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:53,127 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:53,145 INFO - tensorflow - Starting evaluation at 2020-10-05-14:08:53\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:53,215 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:53,284 INFO - tensorflow - Restoring parameters from s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:53,618 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:53,626 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:54,216 INFO - tensorflow - Evaluation [10/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:54,656 INFO - tensorflow - Evaluation [20/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:55,108 INFO - tensorflow - Evaluation [30/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:55,540 INFO - tensorflow - Evaluation [40/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:56,022 INFO - tensorflow - Evaluation [50/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:56,472 INFO - tensorflow - Evaluation [60/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:56,926 INFO - tensorflow - Evaluation [70/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:57,371 INFO - tensorflow - Evaluation [80/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:57,824 INFO - tensorflow - Evaluation [90/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,262 INFO - tensorflow - Evaluation [100/100]\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,289 INFO - tensorflow - Finished evaluation at 2020-10-05-14:08:58\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,289 INFO - tensorflow - Saving dict for global step 1002: accuracy = 0.9862, global_step = 1002, loss = 0.040000297\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.300400: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.300470: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.322324: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.322353: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.342824: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.342858: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,603 INFO - tensorflow - Saving 'checkpoint_path' summary for global step 1002: s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.718926: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.718997: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.740555: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.740586: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.761065: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.761103: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.785011: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.785052: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.807254: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.807324: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.826864: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58.826933: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,904 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[34m('===>', {'inputs': <tf.Tensor 'Placeholder:0' shape=(1, 784) dtype=float32>}, None)\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,973 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,974 INFO - tensorflow - Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,974 INFO - tensorflow - Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,974 INFO - tensorflow - Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,974 INFO - tensorflow - Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:58,974 INFO - tensorflow - Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59,057 INFO - tensorflow - Restoring parameters from s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59,400 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py:1018: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59,400 INFO - tensorflow - Assets added to graph.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59,400 INFO - tensorflow - No assets to write.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59.411938: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59.412018: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59.432707: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59.432788: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59.451502: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:08:59.451531: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:00.699106: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:00.699142: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:00,782 INFO - tensorflow - SavedModel written to: s3://sagemaker-ap-southeast-1-018166606076/sagemaker-tensorflow-2020-10-05-14-03-33-954/checkpoints/export/Servo/temp-1601906938/saved_model.pb\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:00.790847: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:00.790877: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:01.360415: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:01.360459: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:01.379960: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:01.380007: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:01,486 INFO - tensorflow - Loss for final step: 0.010900233.\u001b[0m\n",
      "\u001b[34m2020-10-05 14:09:01,780 INFO - tf_container - Downloaded saved model at /opt/ml/model/export/Servo/1601906938\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-10-05 14:10:17,065 INFO - tf_container - master algo-1 is down, stopping parameter server\u001b[0m\n",
      "\n",
      "2020-10-05 14:10:22 Uploading - Uploading generated training model\n",
      "2020-10-05 14:10:56 Completed - Training job completed\n",
      "Training seconds: 610\n",
      "Billable seconds: 610\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.11.0',\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **```fit```** method will create a training job in two **ml.c4.xlarge** instances. The logs above will show the instances doing training, evaluation, and incrementing the number of **training steps**. \n",
    "\n",
    "In the end of the training, the training job will generate a saved model for TF serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deploy the trained model to prepare for predictions (the old way)\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-ap-southeast-1-018166606076/tensorflow-training-2020-09-26-11-19-40-794/output/model.tar.gz.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b7b00d3481ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n\u001b[0;32m----> 2\u001b[0;31m                                          instance_type='ml.m4.xlarge')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/serving.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sagemaker_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         production_variant = sagemaker.production_variant(\n\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mvpc_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvpc_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0menable_network_isolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_network_isolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         )\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcreate_model_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-ap-southeast-1-018166606076/tensorflow-training-2020-09-26-11-19-40-794/output/model.tar.gz."
     ]
    }
   ],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = np.argmax(predict_response['outputs']['probabilities']['float_val'])\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model using Neo\n",
    "\n",
    "Now the model is ready to be compiled by Neo to be optimized for our hardware of choice. We are using the  ``TensorFlowEstimator.compile_model`` method to do this. For this example, our target hardware is ``'ml_c5'``. You can changed these to other supported target hardware if you prefer.\n",
    "\n",
    "## Compiling the model\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "2.1.0 is the latest version of tensorflow that supports Python 2. Newer versions of tensorflow will only be available for Python 3.Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-018166606076\n",
      "??.....!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The instance type rasp3b is not supported for deployment via SageMaker.Please deploy the model manually.\n"
     ]
    }
   ],
   "source": [
    "output_path = '/'.join(mnist_estimator.output_path.split('/')[:-1])\n",
    "\n",
    "print(output_path)\n",
    "\n",
    "\n",
    "optimized_estimator = mnist_estimator.compile_model(target_instance_family='rasp3b',\n",
    "                              output_path=output_path,\n",
    "                              input_shape= {'inputs':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              framework='tensorflow', framework_version='1.11.0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.c5.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "optimized_predictor.content_type = 'application/vnd+python.numpy+binary'\n",
    "optimized_predictor.serializer = numpy_bytes_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i]\n",
    "    # Display image\n",
    "    im = PIL.Image.fromarray(data.reshape((28,28))*255).convert('L')\n",
    "    display.display(im)\n",
    "    # Invoke endpoint with image\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response\n",
    "    print(\"prediction is {}\".format(np.argmax(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(optimized_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
