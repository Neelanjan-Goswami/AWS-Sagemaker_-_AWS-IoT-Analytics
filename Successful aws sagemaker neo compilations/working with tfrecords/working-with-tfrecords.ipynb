{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with TFRecord Datasets\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Converting a dataset from CSV to TFrecords](#Converting-a-dataset-from-CSV-to-TFrecords)\n",
    " 1. [Upload dataset to S3](#Upload-dataset-to-S3)\n",
    "1. [Construct a DNNClassifier](#Construct-a-DNNClassifier)\n",
    "1. [Train a Model](#Train-a-Model)\n",
    "1. [Run Batch Transform](#Run-Batch-Transform)\n",
    " 1. [Build a container for transforming TFRecord input](#Build-a-container-for-transforming-TFRecord-input)\n",
    " 1. [Push container to ECR](#Push-container-to-ECR)\n",
    " 1. [Create a model with an inference pipeline](#Create-a-model-with-an-inference-pipeline)\n",
    " 1. [Run a batch transform job](#Run-a-batch-transform-job)\n",
    " 1. [Inspect batch transform output](#Inspect-batch-transform-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "TFRecord is a standard TensorFlow data format. It is a record-oriented binary file format that allows for efficient storage and processing of large datasets. In this notebook, we’ll demonstrate how to take an existing CSV dataset and convert it to TFRecord files. We’ll also build a TensorFlow training script that accepts serialized tf.Example protos (the payload of our TFRecords) as input during training. Then, we'll run a training job using the TFRecord dataset we've generated as input. Finally, we'll demonstrate how to run a batch transform job with an inference pipeline so that we can pass the TFRecord dataset as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes you'd like to use for training and batch transform data.\n",
    "* The IAM role that will be used for training and batch transform jobs, as well as ECR repository creation and image upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "\n",
    "bucket = 'irisnew'\n",
    "training_prefix = 'training'\n",
    "batch_input_prefix = 'batch_input'\n",
    "batch_output_prefix ='batch_output'\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a dataset from CSV to TFRecords\n",
    "\n",
    "First, we'll take an existing CSV dataset (located in `./dataset-csv/`) and convert it to the TFRecords file format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "csv_root = './dataset-csv/'\n",
    "tfrecord_root = './dataset-tfrecord/'\n",
    "test_csv_file = 'iris_test.csv'\n",
    "train_csv_file = 'iris_train.csv'\n",
    "test_tfrecord_file = 'iris_test.tfrecords'\n",
    "train_tfrecord_file = 'iris_train.tfrecords'\n",
    "\n",
    "def _floatlist_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[float(value)]))\n",
    "\n",
    "def _int64list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# create the tfrecord dataset dir\n",
    "if not os.path.isdir(tfrecord_root):\n",
    "    os.mkdir(tfrecord_root)\n",
    "\n",
    "for input_file, output_file in [(test_csv_file,test_tfrecord_file), (train_csv_file,train_tfrecord_file)]:\n",
    "    # create the output file\n",
    "    open(tfrecord_root + output_file, 'a').close()\n",
    "    with tf.python_io.TFRecordWriter(tfrecord_root + output_file) as writer:\n",
    "        with open(csv_root + input_file,'r') as f:\n",
    "            f.readline() # skip first line\n",
    "            for line in f:\n",
    "                feature = {\n",
    "                    'sepal_length': _floatlist_feature(line.split(',')[0]),\n",
    "                    'sepal_width': _floatlist_feature(line.split(',')[1]),\n",
    "                    'petal_length': _floatlist_feature(line.split(',')[2]),\n",
    "                    'petal_width': _floatlist_feature(line.split(',')[3]),\n",
    "                }\n",
    "                if f == train_csv_file:\n",
    "                    feature['label'] = _int64list_feature(int(line.split(',')[4].rstrip()))\n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(\n",
    "                        feature=feature\n",
    "                    )\n",
    "                )\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3\n",
    "\n",
    "Next, we'll upload the TFRecord datasets to S3 so that we can use it in training and batch transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket, key, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "    \n",
    "upload_to_s3(bucket, training_prefix + '/' + train_tfrecord_file, tfrecord_root + train_tfrecord_file)\n",
    "upload_to_s3(bucket, batch_input_prefix + '/' + test_tfrecord_file, tfrecord_root + test_tfrecord_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a DNN Classifier\n",
    "\n",
    "In `./dnn-classifier/train.py` we've defined a neural network classifier using TensorFlow's DNNClassifier. We can take a look at the train script to see how the network and input functions are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#     Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n",
      "#\r\n",
      "#     Licensed under the Apache License, Version 2.0 (the \"License\").\r\n",
      "#     You may not use this file except in compliance with the License.\r\n",
      "#     A copy of the License is located at\r\n",
      "#\r\n",
      "#         https://aws.amazon.com/apache-2-0/\r\n",
      "#\r\n",
      "#     or in the \"license\" file accompanying this file. This file is distributed\r\n",
      "#     on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\r\n",
      "#     express or implied. See the License for the specific language governing\r\n",
      "#     permissions and limitations under the License.\r\n",
      "\r\n",
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "import argparse\r\n",
      "import os\r\n",
      "import time\r\n",
      "from os.path import isfile, join\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "\r\n",
      "parser = argparse.ArgumentParser(\r\n",
      "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n",
      "# Data and model checkpoints directories\r\n",
      "parser.add_argument('--data_dir', type=str, default='/opt/ml/input',\r\n",
      "                    help='directory containing dataset')\r\n",
      "parser.add_argument('--model_dir', type=str, default='save',\r\n",
      "                    help='directory to store checkpointed models')\r\n",
      "args = parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "def parser(serialized_example):\r\n",
      "    \"\"\"\r\n",
      "    Parses a single tf.Example into image and label tensors.\r\n",
      "    Should return a (data, labels) tuple where data is a features dict\r\n",
      "    \"\"\"\r\n",
      "    features={\r\n",
      "          'sepal_length': tf.FixedLenFeature([], tf.float32),\r\n",
      "          'sepal_width': tf.FixedLenFeature([], tf.float32),\r\n",
      "          'petal_length': tf.FixedLenFeature([], tf.float32),\r\n",
      "          'petal_width': tf.FixedLenFeature([], tf.float32),\r\n",
      "          'label': tf.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "    parsed_features = tf.parse_single_example(serialized_example, features)\r\n",
      "    features = dict({\r\n",
      "        'sepal_length': tf.cast(parsed_features['sepal_length'], tf.float32),\r\n",
      "        'sepal_width': tf.cast(parsed_features['sepal_width'], tf.float32),\r\n",
      "        'petal_length': tf.cast(parsed_features['petal_length'], tf.float32),\r\n",
      "        'petal_width': tf.cast(parsed_features['petal_width'], tf.float32),\r\n",
      "    })\r\n",
      "    label = tf.cast(parsed_features['label'], tf.int64)\r\n",
      "    return features, label\r\n",
      "\r\n",
      "\r\n",
      "def input_fn_train():\r\n",
      "    \"\"\"Training input function that iterates over TFRecord files in data_dir\"\"\"\r\n",
      "    dataset_files = [f for f in os.listdir(args.data_dir) if isfile(join(args.data_dir, f))]\r\n",
      "\r\n",
      "    dataset = tf.data.TFRecordDataset(dataset_files)\r\n",
      "    dataset = dataset.map(parser)\r\n",
      "    dataset = dataset.batch(1)\r\n",
      "    dataset = dataset.repeat()\r\n",
      "\r\n",
      "    iterator = dataset.make_one_shot_iterator()\r\n",
      "    return iterator.get_next()\r\n",
      "\r\n",
      "\r\n",
      "def serving_input_receiver_fn():\r\n",
      "    \"\"\"A serving input receiver that expects features encoded as JSON \"\"\"\r\n",
      "    features = {\r\n",
      "        'sepal_length': tf.placeholder(tf.float32, [None, 1]),\r\n",
      "        'sepal_width': tf.placeholder(tf.float32, [None, 1]),\r\n",
      "        'petal_length': tf.placeholder(tf.float32, [None, 1]),\r\n",
      "        'petal_width': tf.placeholder(tf.float32, [None, 1]),\r\n",
      "    }\r\n",
      "    return tf.estimator.export.ServingInputReceiver(features, features)\r\n",
      "\r\n",
      "\r\n",
      "def train(args):\r\n",
      "    feature_columns = [tf.feature_column.numeric_column(key=\"sepal_length\", dtype=tf.float32),\r\n",
      "                          tf.feature_column.numeric_column(key=\"sepal_width\", dtype=tf.float32),\r\n",
      "                          tf.feature_column.numeric_column(key=\"petal_length\", dtype=tf.float32),\r\n",
      "                          tf.feature_column.numeric_column(key=\"petal_width\", dtype=tf.float32)]\r\n",
      "\r\n",
      "    estimator =  tf.estimator.DNNClassifier(feature_columns=feature_columns,\r\n",
      "                                            hidden_units=[10, 20, 10],\r\n",
      "                                            n_classes=3)\r\n",
      "\r\n",
      "    estimator.train(input_fn=input_fn_train, steps=1000)\r\n",
      "\r\n",
      "    print(f'Saving model to {args.model_dir}')\r\n",
      "    estimator.export_saved_model(args.model_dir, serving_input_receiver_fn=serving_input_receiver_fn)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./dnn-classifier/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model\n",
    "\n",
    "Next, we'll kick off a training job using the training script defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-29 13:56:16 Starting - Starting the training job...\n",
      "2020-09-29 13:56:19 Starting - Launching requested ML instances......\n",
      "2020-09-29 13:57:20 Starting - Preparing the instances for training...\n",
      "2020-09-29 13:58:04 Downloading - Downloading input data...\n",
      "2020-09-29 13:58:36 Training - Training image download completed. Training in progress..\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:40,395 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:40,400 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:40,788 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:40,803 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:40,818 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:40,828 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"/opt/ml/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2020-09-29-13-56-15-687\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-018166606076/tensorflow-training-2020-09-29-13-56-15-687/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-018166606076/tensorflow-training-2020-09-29-13-56-15-687/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2020-09-29-13-56-15-687\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-018166606076/tensorflow-training-2020-09-29-13-56-15-687/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --model_dir /opt/ml/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1t81m1ou\u001b[0m\n",
      "\u001b[34m[2020-09-29 13:58:43.943 ip-10-0-95-208.ap-southeast-1.compute.internal:25 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-09-29 13:58:43.944 ip-10-0-95-208.ap-southeast-1.compute.internal:25 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-09-29 13:58:43.944 ip-10-0-95-208.ap-southeast-1.compute.internal:25 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:66: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:66: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-09-29 13:58:44.857 ip-10-0-95-208.ap-southeast-1.compute.internal:25 INFO hook.py:326] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp1t81m1ou/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp1t81m1ou/model.ckpt.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/smdebug/tensorflow/session.py:310: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/smdebug/tensorflow/session.py:310: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loss for final step: None.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loss for final step: None.\u001b[0m\n",
      "\u001b[34mSaving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:73: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:73: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'sepal_length': <tf.Tensor 'Placeholder:0' shape=(?, 1) dtype=float32>, 'sepal_width': <tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>, 'petal_length': <tf.Tensor 'Placeholder_2:0' shape=(?, 1) dtype=float32>, 'petal_width': <tf.Tensor 'Placeholder_3:0' shape=(?, 1) dtype=float32>}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'sepal_length': <tf.Tensor 'Placeholder:0' shape=(?, 1) dtype=float32>, 'sepal_width': <tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>, 'petal_length': <tf.Tensor 'Placeholder_2:0' shape=(?, 1) dtype=float32>, 'petal_width': <tf.Tensor 'Placeholder_3:0' shape=(?, 1) dtype=float32>}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'sepal_length': <tf.Tensor 'Placeholder:0' shape=(?, 1) dtype=float32>, 'sepal_width': <tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>, 'petal_length': <tf.Tensor 'Placeholder_2:0' shape=(?, 1) dtype=float32>, 'petal_width': <tf.Tensor 'Placeholder_3:0' shape=(?, 1) dtype=float32>}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'sepal_length': <tf.Tensor 'Placeholder:0' shape=(?, 1) dtype=float32>, 'sepal_width': <tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>, 'petal_length': <tf.Tensor 'Placeholder_2:0' shape=(?, 1) dtype=float32>, 'petal_width': <tf.Tensor 'Placeholder_3:0' shape=(?, 1) dtype=float32>}\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from /tmp/tmp1t81m1ou/model.ckpt-0\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from /tmp/tmp1t81m1ou/model.ckpt-0\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-b'1601387926'/saved_model.pb\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-b'1601387926'/saved_model.pb\u001b[0m\n",
      "\u001b[34m[2020-09-29 13:58:46.342 ip-10-0-95-208.ap-southeast-1.compute.internal:25 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-09-29 13:58:46,711 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-09-29 13:58:59 Uploading - Uploading generated training model\n",
      "2020-09-29 13:58:59 Completed - Training job completed\n",
      "Training seconds: 55\n",
      "Billable seconds: 55\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_data_location = 's3://{}/{}'.format(bucket, training_prefix)\n",
    "instance_type = 'ml.c4.xlarge'\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='dnn-classifier',\n",
    "                       model_dir='/opt/ml/model',\n",
    "                       train_instance_type=instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       role=sagemaker.get_execution_role(), # Passes to the container the AWS role that you are using on this notebook\n",
    "                       framework_version='1.15.0', # Uses TensorFlow 1.11\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)\n",
    "\n",
    "inputs = {'training': train_data_location}\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Model using Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://irisnew/compiledmodel/output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??...*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Compilation job compilation-tensorflow-training-2020-10-05-13-52-36-381: Failed. Reason: ClientError: InputConfiguration: TVM cannot convert Tensorflow model. Please make sure the framework you selected is correct. The following operators are not implemented: {'AsString'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-800b515c1280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Batch size 1, 3 channels, 224x224 Images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                               framework='tensorflow', framework_version='1.15.0')\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(self, target_instance_family, input_shape, output_path, framework, framework_version, compile_max_run, tags, target_platform_os, target_platform_arch, target_platform_accelerator, compiler_options, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mtarget_platform_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_platform_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mtarget_platform_accelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_platform_accelerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m             \u001b[0mcompiler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompiler_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         )\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_instance_family\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, target_instance_family, input_shape, output_path, role, tags, job_name, compile_max_run, framework, framework_version, target_platform_os, target_platform_arch, target_platform_accelerator, compiler_options)\u001b[0m\n\u001b[1;32m    434\u001b[0m         )\n\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mjob_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_compilation_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_status\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ModelArtifacts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"S3ModelArtifacts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_instance_family\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_compilation_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   2575\u001b[0m         \"\"\"\n\u001b[1;32m   2576\u001b[0m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_compilation_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2577\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CompilationJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2578\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                 ),\n\u001b[1;32m   2670\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             )\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Compilation job compilation-tensorflow-training-2020-10-05-13-52-36-381: Failed. Reason: ClientError: InputConfiguration: TVM cannot convert Tensorflow model. Please make sure the framework you selected is correct. The following operators are not implemented: {'AsString'}"
     ]
    }
   ],
   "source": [
    "output_path = \"s3://irisnew/compiledmodel/output\"\n",
    "print(output_path)\n",
    "optimized_estimator = estimator.compile_model(target_instance_family='rasp3b', \n",
    "                              target_platform_os=\"LINUX\",\n",
    "                              target_platform_arch=\"ARM_EABIHF\",\n",
    "                              compiler_options={'mattr': ['+neon']},\n",
    "                              input_shape={'features':[4]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.15.0')\n",
    "\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container for transforming TFRecord input\n",
    "\n",
    "The SageMaker TensorFlow Serving container uses the TensorFlow ModelServer RESTful API to serve predict requests. In the next step, we'll create a container to transform mini-batch TFRecord payloads into JSON objects that can be forwarded to the TensorFlow serving container. To do this, we've created a simple Python Flask app that does the transformation, the code for this container is available in the `./tfrecord-transformer-container/` directory. First, we'll build the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t tfrecord-transformer ./tfrecord-transformer-container/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push container to ECR\n",
    "\n",
    "Next, we'll push the docker container to an ECR repository in your account. In order to push the container to ECR, the execution role attached to this notebook should have permissions to create a repository, set a repository policy, and upload an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'tfrecord-transformer'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "transformer_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# docker login\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "# create ecr repository\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "# attach policy allowing sagemaker to pull this image\n",
    "!aws ecr set-repository-policy --repository-name $ecr_repository --policy-text \"$( cat ./tfrecord-transformer-container/ecr_policy.json )\"\n",
    "\n",
    "!docker tag {ecr_repository + tag} $transformer_repository_uri\n",
    "!docker push $transformer_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model with an inference pipeline\n",
    "\n",
    "Next, we'll create a SageMaker model with the two containers chained together (TFRecord transformer -> TensorFlow Serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "model_name = name_from_base('tfrecord-to-tfserving')\n",
    "\n",
    "transform_container = {\n",
    "    \"Image\": transformer_repository_uri\n",
    "}\n",
    "\n",
    "tf_serving_model = Model(model_data=estimator.model_data,\n",
    "                         role=sagemaker.get_execution_role(),\n",
    "                         image=estimator.image_name,\n",
    "                         framework_version=estimator.framework_version,\n",
    "                         sagemaker_session=estimator.sagemaker_session)\n",
    "tf_serving_container = tf_serving_model.prepare_container_def(instance_type)\n",
    "\n",
    "model_params = {\n",
    "    \"ModelName\": model_name,\n",
    "    \"Containers\": [\n",
    "        transform_container,\n",
    "        tf_serving_container\n",
    "    ],\n",
    "    \"ExecutionRoleArn\": sagemaker.get_execution_role()\n",
    "}\n",
    "\n",
    "client.create_model(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a batch transform job\n",
    "\n",
    "Next, we'll run a batch transform job using our inference pipeline model. We'll specify `SplitType=TFRecord` and `BatchStrategy=MultiRecord` to specify that our dataset will be split by TFRecord boundaries, and multiple records will be batched in a single request up to the `MaxPayloadInMB=1` limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = 's3://{}/{}'.format(bucket, batch_input_prefix)\n",
    "output_data_path = 's3://{}/{}'.format(bucket, batch_output_prefix)\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    strategy = 'MultiRecord',\n",
    "    max_payload = 1,\n",
    "    output_path = output_data_path,\n",
    "    assemble_with= 'Line',\n",
    "    base_transform_job_name='tfrecord-transform',\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "transformer.transform(data = input_data_path,\n",
    "                      split_type = 'TFRecord')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect batch transform output\n",
    "\n",
    "Finally, we can inspect the output files of our batch transform job to see the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_uri = transformer.output_path + '/' + test_tfrecord_file + '.out'\n",
    "!aws s3 cp $output_uri -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
